@article{Andriluka,
author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt and Planck, Max},
file = {::},
title = {{2D Human Pose Estimation : New Benchmark and State of the Art Analysis : Supplementary Material}}
}
@article{Wei2016,
abstract = {Pose Machines provide a powerful modular framework for articulated pose estimation. The sequential prediction framework allows for the learning of rich implicit spatial models, but currently relies on manually designed features for representing image and spatial context. In this work, we incorporate a convolutional network architecture into the pose machine framework allowing the learning of representations for both image and spatial context directly from data. The contribution of this paper is a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks. Our approach ad- dresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing backpropagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.00134},
author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
eprint = {1602.00134},
file = {::},
title = {{Convolutional Pose Machines}},
url = {http://arxiv.org/abs/1602.00134},
year = {2016}
}
@article{Lenc2015,
abstract = {Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection.},
archivePrefix = {arXiv},
arxivId = {1506.06981},
author = {Lenc, Karel and Vedaldi, Andrea},
doi = {10.5244/C.29.5},
eprint = {1506.06981},
file = {::},
isbn = {1-901725-53-7},
journal = {Procedings of the British Machine Vision Conference 2015},
pages = {5.1--5.12},
title = {{R-CNN minus R}},
url = {http://arxiv.org/abs/1506.06981$\backslash$nhttp://bmvc2015.swansea.ac.uk/proceedings/papers/paper005/$\backslash$nhttp://www.bmva.org/bmvc/2015/papers/paper005/index.html},
year = {2015}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
eprint = {arXiv:1506.01497v1},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
journal = {ArXiv 2015},
pages = {1--10},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1{\%}, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {::},
journal = {arXiv preprint},
pages = {1--11},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {::},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{Srivastava2015,
abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1507.06228},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
eprint = {1507.06228},
file = {::},
journal = {arXiv},
pages = {1--9},
title = {{Training Very Deep Networks}},
url = {http://arxiv.org/abs/1507.06228},
year = {2015}
}
@article{Gkioxari2015,
abstract = {There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose which is characteristic for the action, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we ex-ploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition sys-tem. We adapt RCNN to use more than one region for classi-fication while still maintaining the ability to localize the ac-tion. We call our system R * CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R * CNN achieves 89{\%} mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.01197v1},
author = {Gkioxari, Georgia and Berkeley, U C and Girshick, Ross and Berkeley, U C},
doi = {10.1109/ICCV.2015.129},
eprint = {arXiv:1505.01197v1},
file = {::},
journal = {Cvpr},
pages = {1080--1088},
title = {{Contextual Action Recognition with R*CNN}},
year = {2015}
}
@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {::},
journal = {None},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {::},
journal = {Arvix},
pages = {1520--1528},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {http://arxiv.org/abs/1505.04366},
volume = {1},
year = {2015}
}
@article{Tompson2015,
abstract = {Convolutional Neural Network를 이용하여 사람 joint(관절)의 location을 estimate 하는 법 제안},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4280v1},
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and Lecun, Yann and Bregler, Christoph},
doi = {10.1109/CVPR.2015.7298664},
eprint = {arXiv:1411.4280v1},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tompson et al. - 2015 - Efficient Object Localization Using Convolutional Networks.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
pages = {2014},
title = {{Efficient Object Localization Using Convolutional Networks}},
year = {2015}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error and 17.3{\%} top-1 error.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {1512.00567},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
journal = {arXiv preprint},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
journal = {arXiv},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {::},
isbn = {978-3-319-10577-2},
issn = {0162-8828},
journal = {arXiv preprint arXiv {\ldots}},
pages = {346--361},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729v1$\backslash$npapers3://publication/uuid/09415D06-A785-4329-82C0-B9FF2B1FEAB7},
volume = {cs.CV},
year = {2014}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v1},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v1},
file = {::},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Long2014,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2014 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {978-1-4673-6964-0},
journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {3431--3440},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298965$\backslash$nhttp://arxiv.org/abs/1411.4038},
year = {2014}
}
@article{Girshick2014,
abstract = {—Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50{\%} relative to the previous best result on VOC 2012—achieving a mAP of 62.4{\%}. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/TPAMI.2015.2437384},
eprint = {1311.2524},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Region-based Convolutional Networks for Accurate Object Detection and Segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {0162-8828},
journal = {Pami},
keywords = {Convolutional Networks,Deep Learning,Detection,Index Terms—Object Recognition,Semantic Segmentation,Transfer Learning !},
number = {1},
pages = {1--16},
pmid = {26656583},
title = {{Region-based Convolutional Networks for Accurate Object Detection and Segmentation}},
url = {http://www.cs.berkeley.edu/},
volume = {38},
year = {2014}
}
@article{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {1550-5499},
journal = {arXiv preprint arXiv:1409.4842},
pages = {1--12},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842v1},
year = {2014}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Uijlings2013,
abstract = {For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7{\%} of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5{\%} for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Uijlings, J. R R and {Van De Sande}, K. E A and Gevers, T. and Smeulders, A. W M},
doi = {10.1007/s11263-013-0620-5},
eprint = {1409.4842},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - 2013 - Selective search for object recognition.pdf:pdf},
isbn = {9781457711015},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {154--171},
title = {{Selective search for object recognition}},
volume = {104},
year = {2013}
}
@article{Bengio2012,
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {::},
isbn = {0162-8828 VO - 35},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation Learning: A Review and New Perspectives}},
volume = {35},
year = {2012}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sulskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
