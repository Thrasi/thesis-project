@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {::},
journal = {None},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Andriluka,
author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt and Planck, Max},
file = {::},
title = {{2D Human Pose Estimation : New Benchmark and State of the Art Analysis : Supplementary Material}}
}
@article{Bengio2012,
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {::},
isbn = {0162-8828 VO - 35},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation Learning: A Review and New Perspectives}},
volume = {35},
year = {2012}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {::},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Farabet2013,
annote = {claims: state-of-the-art performance

same convolutional network applied at 3 scales. small scale output is upscaled. Outputs are concatenated to a per pixel prediction. Boundaries are not accurately pinpointed.

Then they try 3 different graph based classifications on top of the pixel predictions.
a) majority vote within a superpixel
b) CRF over super pixels
c) higherarchy of superpixels


single network multi scale, shared weights learn to capture long range interactions of pixels.


shallow net 3 layers.},
author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
doi = {10.1109/TPAMI.2012.231},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farabet et al. - 2013 - Learning Hierarchical Features for Scence Labeling(2).pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {1--15},
pmid = {23787344},
title = {{Learning Hierarchical Features for Scence Labeling}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6338939},
year = {2013}
}
@article{Felzenszwalb2005,
abstract = {The goal of this paper is to recognize various deformable objects from$\backslash$r$\backslash$nimages. To this end we extend the class of generative probabilistic$\backslash$r$\backslash$nmodels known as pictorial structures. This class of models is$\backslash$r$\backslash$nparticularly suited to represent articulated structures, and has$\backslash$r$\backslash$npreviously been used by Felzenszwalb and Huttenlocher for pose$\backslash$r$\backslash$nestimation of humans.  We extend pictorial structures in three ways:$\backslash$r$\backslash$n(i) likelihoods are included for both the boundary and the enclosed$\backslash$r$\backslash$ntexture of the animal; (ii) a complete graph is modelled (rather than$\backslash$r$\backslash$na tree structure); (iii) it is demonstrated that the model can be$\backslash$r$\backslash$nfitted in polynomial time using belief propagation.$\backslash$r$\backslash$n$\backslash$r$\backslash$nWe show examples for two types of quadrupeds, cows and horses.  We$\backslash$r$\backslash$nachieve excellent recognition performance for cows with an equal error$\backslash$r$\backslash$nrate of 3{\%} for 500 positive and 5000 negative images.},
author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
doi = {10.1023/B:VISI.0000042934.15159.49},
file = {:home/mb/Downloads/pict-struct-ijcv.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Energy minimization,Part-based object recognition,Statistical models},
number = {1},
pages = {55--79},
title = {{Pictorial structures for object recognition}},
volume = {61},
year = {2005}
}
@article{Girshick2014,
abstract = {—Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50{\%} relative to the previous best result on VOC 2012—achieving a mAP of 62.4{\%}. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/TPAMI.2015.2437384},
eprint = {1311.2524},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Region-based Convolutional Networks for Accurate Object Detection and Segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {0162-8828},
journal = {Pami},
keywords = {Convolutional Networks,Deep Learning,Detection,Index Terms—Object Recognition,Semantic Segmentation,Transfer Learning !},
number = {1},
pages = {1--16},
pmid = {26656583},
title = {{Region-based Convolutional Networks for Accurate Object Detection and Segmentation}},
url = {http://www.cs.berkeley.edu/},
volume = {38},
year = {2014}
}
@article{Gkioxari2015,
abstract = {There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose which is characteristic for the action, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we ex-ploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition sys-tem. We adapt RCNN to use more than one region for classi-fication while still maintaining the ability to localize the ac-tion. We call our system R * CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R * CNN achieves 89{\%} mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.01197v1},
author = {Gkioxari, Georgia and Berkeley, U C and Girshick, Ross and Berkeley, U C},
doi = {10.1109/ICCV.2015.129},
eprint = {arXiv:1505.01197v1},
file = {::},
journal = {Cvpr},
pages = {1080--1088},
title = {{Contextual Action Recognition with R*CNN}},
year = {2015}
}
@article{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1{\%}, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {::},
journal = {arXiv preprint},
pages = {1--11},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {::},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {::},
isbn = {978-3-319-10577-2},
issn = {0162-8828},
journal = {arXiv preprint arXiv {\ldots}},
pages = {346--361},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729v1$\backslash$npapers3://publication/uuid/09415D06-A785-4329-82C0-B9FF2B1FEAB7},
volume = {cs.CV},
year = {2014}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
journal = {arXiv},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sulskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Lenc2015,
abstract = {Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection.},
archivePrefix = {arXiv},
arxivId = {1506.06981},
author = {Lenc, Karel and Vedaldi, Andrea},
doi = {10.5244/C.29.5},
eprint = {1506.06981},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lenc, Vedaldi - 2015 - R-CNN minus R.pdf:pdf},
isbn = {1-901725-53-7},
journal = {Procedings of the British Machine Vision Conference 2015},
pages = {5.1--5.12},
title = {{R-CNN minus R}},
url = {http://arxiv.org/abs/1506.06981$\backslash$nhttp://bmvc2015.swansea.ac.uk/proceedings/papers/paper005/$\backslash$nhttp://www.bmva.org/bmvc/2015/papers/paper005/index.html},
year = {2015}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v1},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v1},
file = {::},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Long2014,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2014 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {978-1-4673-6964-0},
journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {3431--3440},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298965$\backslash$nhttp://arxiv.org/abs/1411.4038},
year = {2014}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noh, Hong, Han - 2015 - Learning Deconvolution Network for Semantic Segmentation.pdf:pdf},
journal = {Arvix},
pages = {1520--1528},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {http://arxiv.org/abs/1505.04366},
volume = {1},
year = {2015}
}
@article{Ramakrishna2014,
abstract = {State-of-the-art approaches for articulated human pose es-timation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine frame-work and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation with-out specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks.},
author = {Ramakrishna, Varun and Munoz, Daniel and Hebert, Martial and {Andrew Bagnell}, James and Sheikh, Yaser},
doi = {10.1007/978-3-319-10605-2_3},
file = {:home/mb/Downloads/poseMachines.pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {33--47},
title = {{Pose machines: Articulated pose estimation via inference machines}},
volume = {8690 LNCS},
year = {2014}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
eprint = {arXiv:1506.01497v1},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
journal = {ArXiv 2015},
pages = {1--10},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Srivastava2015,
abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1507.06228},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
eprint = {1507.06228},
file = {::},
journal = {arXiv},
pages = {1--9},
title = {{Training Very Deep Networks}},
url = {http://arxiv.org/abs/1507.06228},
year = {2015}
}
@article{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {1550-5499},
journal = {arXiv preprint arXiv:1409.4842},
pages = {1--12},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842v1},
year = {2014}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error and 17.3{\%} top-1 error.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {1512.00567},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
journal = {arXiv preprint},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Tompson2015,
abstract = {Convolutional Neural Network를 이용하여 사람 joint(관절)의 location을 estimate 하는 법 제안},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4280v1},
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and Lecun, Yann and Bregler, Christoph},
doi = {10.1109/CVPR.2015.7298664},
eprint = {arXiv:1411.4280v1},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tompson et al. - 2015 - Efficient Object Localization Using Convolutional Networks.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
pages = {2014},
title = {{Efficient Object Localization Using Convolutional Networks}},
year = {2015}
}
@article{Tompson2014,
abstract = {This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1406.2984},
author = {Tompson, Jonathan and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
eprint = {1406.2984},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tompson et al. - 2014 - Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation.pdf:pdf},
issn = {10495258},
journal = {Nips 2014},
pages = {1--9},
title = {{Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation}},
year = {2014}
}
@article{Uijlings2013,
abstract = {For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7{\%} of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5{\%} for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Uijlings, J. R R and {Van De Sande}, K. E A and Gevers, T. and Smeulders, A. W M},
doi = {10.1007/s11263-013-0620-5},
eprint = {1409.4842},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - 2013 - Selective search for object recognition.pdf:pdf},
isbn = {9781457711015},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {154--171},
title = {{Selective search for object recognition}},
volume = {104},
year = {2013}
}
@article{Wei2016,
abstract = {Pose Machines provide a powerful modular framework for articulated pose estimation. The sequential prediction framework allows for the learning of rich implicit spatial models, but currently relies on manually designed features for representing image and spatial context. In this work, we incorporate a convolutional network architecture into the pose machine framework allowing the learning of representations for both image and spatial context directly from data. The contribution of this paper is a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks. Our approach ad- dresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing backpropagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.00134},
author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
eprint = {1602.00134},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei et al. - 2016 - Convolutional Pose Machines(2).pdf:pdf},
title = {{Convolutional Pose Machines}},
url = {http://arxiv.org/abs/1602.00134},
year = {2016}
}
@article{Yang2011,
abstract = {We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50{\%} while being orders of magnitude faster.},
annote = {claims: 50{\%} better than state-of-the-art, orders of magnitude faste.

use : "simple represen- tation for modeling a family of affinely-warped templates: a mixture of non-oriented pictorial structures (Fig.1)." basically vertical and horizontal springs.

Use relative locations of joints as cues for orientation. E.g. downwards facing hand appears below elbows.},
author = {Yang, Yi and Ramanan, Deva},
doi = {10.1109/CVPR.2011.5995741},
file = {:home/mb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Ramanan - 2011 - Articulated pose estimation with flexible mixtures-of-parts(2).pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1385--1392},
pmid = {23248128},
title = {{Articulated pose estimation with flexible mixtures-of-parts}},
year = {2011}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:home/mb/Downloads/1504.08083v2.pdf:pdf},
isbn = {978-1-4673-8391-2},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
year = {2015}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Sak, Ha$\backslash$csim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
eprint = {arXiv:1402.1128v1},
file = {:home/mb/Downloads/1402.1128v1.pdf:pdf},
journal = {arXiv preprint arXiv:1402.1128},
number = {Cd},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
year = {2014}
}
@article{Ladick??2009,
abstract = {Most methods for object class segmentation are formulated as a labelling problem over a single choice of quantisation of an image space - pixels, segments or group of segments. It is well known that each quantisation has its fair share of pros and cons; and the existence of a common optimal quantisation level suitable for all object categories is highly unlikely. Motivated by this observation, we propose a hierarchical random field model, that allows integration of features computed at different levels of the quantisation hierarchy. MAP inference in this model can be performed efficiently using powerful graph cut based move making algorithms. Our framework generalises much of the previous work based on pixels or segments. We evaluate its efficiency on some of the most challenging data-sets for object class segmentation, and show it obtains state-of-the-art results.},
author = {Ladick??, L'ubor and Russell, Chris and Kohli, Pushmeet and Torr, Philip H S},
doi = {10.1109/ICCV.2009.5459248},
file = {:home/mb/Downloads/iccv09.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {739--746},
pmid = {5459248},
title = {{Associative hierarchical CRFs for object class image segmentation}},
year = {2009}
}
